"""
Apache HTTP ì„œë²„ ì›¹ í˜ì´ì§€ íŒŒì‹±ì„ ìœ„í•œ
ëª¨ë“ˆ
"""

import httpx
import asyncio
import fnmatch
import time
import sys
import os
from functools import lru_cache
from bs4 import BeautifulSoup
from typing import List, Optional
import typer
from rich.console import Console
from rich.table import Table
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TaskProgressColumn
from rich.panel import Panel
from rich.tree import Tree
from rich import print as rprint

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ íŒŒì´ì¬ ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../../..')))
from src.mt_dataset_cli.crawler.schemas import WebParser, FileSystemEntry

# Rich ì½˜ì†” ê°ì²´ ìƒì„±
console = Console()

class ApacheHttpServerParser(WebParser):
    """
    Apache HTTP ì„œë²„ ë””ë ‰í† ë¦¬ ë¦¬ìŠ¤íŒ… í˜ì´ì§€ë¥¼ íŒŒì‹±í•˜ëŠ” í´ë˜ìŠ¤
    
    WebParserë¥¼ ìƒì†ë°›ì•„ Apache ìŠ¤íƒ€ì¼ì˜ ë””ë ‰í† ë¦¬ ë¦¬ìŠ¤íŒ… í˜ì´ì§€ì—ì„œ
    íŒŒì¼ê³¼ ë””ë ‰í† ë¦¬ ë§í¬ë¥¼ ì¶”ì¶œí•˜ëŠ” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
    """

    def __init__(self, url: str, max_retries: int = 3, retry_delay: float = 1.0):
        """
        Args:
            url (str): íŒŒì‹±í•  ì›¹ í˜ì´ì§€ì˜ URL
            max_retries (int): ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
            retry_delay (float): ì¬ì‹œë„ ê°„ ëŒ€ê¸° ì‹œê°„(ì´ˆ)
        """
        super().__init__(url)
        self.max_retries = max_retries
        self.retry_delay = retry_delay

    @lru_cache(maxsize=1000)
    def fetch(self) -> str:
        """
        ì›¹ í˜ì´ì§€ì˜ ë‚´ìš©ì„ ê°€ì ¸ì˜µë‹ˆë‹¤. ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„í•©ë‹ˆë‹¤.
        
        Args:
            url (str): ê°€ì ¸ì˜¬ ì›¹ í˜ì´ì§€ì˜ URL
            
        Returns:
            str: ì›¹ í˜ì´ì§€ì˜ HTML ë‚´ìš©
            
        Raises:
            httpx.HTTPError: ëª¨ë“  ì¬ì‹œë„ í›„ì—ë„ HTTP ìš”ì²­ì´ ì‹¤íŒ¨í•œ ê²½ìš°
        """
        retries = 0
        while retries < self.max_retries:
            try:
                response = httpx.get(self.url, follow_redirects=True, timeout=30.0)
                response.raise_for_status()
                return response.text
            except httpx.HTTPError as e:
                retries += 1
                if retries == self.max_retries:
                    raise
                time.sleep(self.retry_delay * retries)  # ì§€ìˆ˜ ë°±ì˜¤í”„

    def parse(self) -> List[FileSystemEntry]:
        """
        ì›¹ í˜ì´ì§€ë¥¼ íŒŒì‹±í•˜ì—¬ Apache ë””ë ‰í† ë¦¬ ë¦¬ìŠ¤íŒ…ì˜ ìš”ì†Œë“¤ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        Returns:
            List[FileSystemEntry]: íŒŒì‹±ëœ íŒŒì¼/ë””ë ‰í† ë¦¬ ìš”ì†Œ ë¦¬ìŠ¤íŠ¸
        """
        content = self.fetch()
        soup = BeautifulSoup(content, 'lxml')

        elements = []
        rows = soup.select("body > table tr")[3:-1]  # í—¤ë”ì™€ í‘¸í„° ì œì™¸
        for row in rows:
            cols = row.find_all('td')
            if len(cols) >= 4:  # Name, Last modified, Size, Description ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸
                anchor = cols[1].find('a')
                if anchor:
                    filename = anchor.text
                    url = anchor.get('href')
                    url = url if url.startswith('http') else self.url.rstrip('/') + '/' + url
                else:
                    filename = ''
                    url = ''

                type_ = "directory" if url.endswith('/') else "file"
                last_modified = cols[2].text.strip()
                size = cols[3].text.strip()
                description = cols[4].text.strip()
                
                element = FileSystemEntry(
                    url=url,
                    filename=filename,
                    last_modified=last_modified,
                    size=size,
                    description=description,
                    type=type_
                )
                elements.append(element)
                
        return elements

    async def walk(self, glob_pattern: str = None, visited_urls=None, depth=0, max_depth=5):
        """
        ì›¹ í˜ì´ì§€ë¥¼ ì¬ê·€ì ìœ¼ë¡œ íƒìƒ‰í•˜ì—¬ ëª¨ë“  íŒŒì¼ê³¼ ë””ë ‰í† ë¦¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        Args:
            glob_pattern: íŒŒì¼ëª… í•„í„°ë§ì„ ìœ„í•œ glob íŒ¨í„´ (ì˜ˆ: "*.txt", "data-*.tgz")
            visited_urls: ì´ë¯¸ ë°©ë¬¸í•œ URL ì§‘í•© (ë¬´í•œ ë£¨í”„ ë°©ì§€)
            depth: í˜„ì¬ íƒìƒ‰ ê¹Šì´
            max_depth: ìµœëŒ€ íƒìƒ‰ ê¹Šì´
            
        Returns:
            ë¹„ë™ê¸° ì œë„ˆë ˆì´í„°: ë°œê²¬ëœ íŒŒì¼ ë˜ëŠ” ë””ë ‰í† ë¦¬ í•­ëª©ì„ ìƒì„±í•˜ëŠ” ë¹„ë™ê¸° ì œë„ˆë ˆì´í„°
        """
        # ë°©ë¬¸í•œ URL ê´€ë¦¬ (ë¬´í•œ ë£¨í”„ ë°©ì§€)
        if visited_urls is None:
            visited_urls = set()
        
        # ìµœëŒ€ ê¹Šì´ ì œí•œ í™•ì¸
        if depth > max_depth:
            return
            
        # ì´ë¯¸ ë°©ë¬¸í•œ URL í™•ì¸
        if self.url in visited_urls:
            return
            
        # í˜„ì¬ URL ë°©ë¬¸ ì²˜ë¦¬
        visited_urls.add(self.url)
        
        try:
            # í˜„ì¬ ë””ë ‰í† ë¦¬ì—ì„œ ëª¨ë“  ìš”ì†Œ íŒŒì‹±
            elements = self.parse()
            
            # í•„í„°ë§ í•¨ìˆ˜
            def matches_pattern(filename):
                if not glob_pattern:
                    return True
                return fnmatch.fnmatch(filename, glob_pattern)
            
            # ì¼ë‹¨ ë””ë ‰í† ë¦¬ë¥¼ í•œ ë²ˆì— ëª¨ì•„ì„œ ë‚˜ì¤‘ì— ì²˜ë¦¬ (ë™ì‹œì„± í™•ë³´)
            directories = []
            
            # íŒŒì¼ ë¨¼ì € ì²˜ë¦¬
            for element in elements:
                # íŒŒì¼ë§Œ ë¨¼ì € ì²˜ë¦¬í•˜ê³  ë§¤ì¹­ë˜ëŠ” ê²ƒë§Œ ë°˜í™˜
                if element.type == "file" and matches_pattern(element.filename):
                    yield element
                elif element.type == "directory":
                    # ë””ë ‰í† ë¦¬ ìì²´ê°€ íŒ¨í„´ì— ë§¤ì¹­ë˜ë©´ ë°˜í™˜
                    if matches_pattern(element.filename):
                        yield element
                    directories.append(element)
            
            # ë””ë ‰í† ë¦¬ë¥¼ ë¹„ë™ê¸°ì ìœ¼ë¡œ ë³‘ë ¬ ì²˜ë¦¬
            if directories:
                tasks = []
                for dir_element in directories:
                    # í•˜ìœ„ ë””ë ‰í† ë¦¬ íƒìƒ‰ ì‘ì—… ìƒì„±
                    parser = ApacheHttpServerParser(dir_element.url, max_retries=self.max_retries, retry_delay=self.retry_delay)
                    tasks.append(parser.walk(
                        glob_pattern=glob_pattern,
                        visited_urls=visited_urls,
                        depth=depth+1,
                        max_depth=max_depth
                    ))
                
                # ë³‘ë ¬ë¡œ ëª¨ë“  í•˜ìœ„ ë””ë ‰í† ë¦¬ íƒìƒ‰ (gather ì‚¬ìš©)
                if tasks:
                    async_generators = await asyncio.gather(*[anext(task, None) for task in tasks])
                    
                    # ëª¨ë“  ì œë„ˆë ˆì´í„°ì—ì„œ í•­ëª© ê°€ì ¸ì˜¤ê¸°
                    for i, task in enumerate(tasks):
                        # ì²« ë²ˆì§¸ í•­ëª©ì´ ìˆìœ¼ë©´ ë°˜í™˜
                        if async_generators[i] is not None:
                            yield async_generators[i]
                        
                        # ë‚˜ë¨¸ì§€ í•­ëª©ë“¤ ë°˜í™˜
                        try:
                            async for item in task:
                                yield item
                        except Exception as e:
                            console.print(f"[bold red][ì˜¤ë¥˜][/] í•˜ìœ„ ë””ë ‰í† ë¦¬ íƒìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                    
        except Exception as e:
            console.print(f"[bold red][ì˜¤ë¥˜][/] URL '{self.url}' íƒìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

# Typer ì•± ìƒì„±
app = typer.Typer(
    help="Apache HTTP Server ë””ë ‰í† ë¦¬ í¬ë¡¤ëŸ¬",
    add_completion=False,
)

async def crawl_and_display(url: str, pattern: str, max_depth: int, max_display: int, 
                            output_format: str, save_to_file: Optional[str] = None):
    """í¬ë¡¤ë§ ë° ê²°ê³¼ í‘œì‹œ í•¨ìˆ˜"""
    # í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    http_parser = ApacheHttpServerParser(url)
    
    # ë£¨íŠ¸ ë§í¬ ê°€ì ¸ì˜¤ê¸°
    with console.status("[bold green]ë£¨íŠ¸ ë””ë ‰í† ë¦¬ ë¶„ì„ ì¤‘...[/]", spinner="dots"):
        links = http_parser.parse()
    
    # ë£¨íŠ¸ ë§í¬ í‘œì‹œ
    console.print(Panel(
        f"[bold]ë£¨íŠ¸ ë””ë ‰í† ë¦¬ì—ì„œ ë°œê²¬ëœ ë§í¬ ìˆ˜: {len(links)}[/]", 
        title="[green]ë£¨íŠ¸ ë””ë ‰í† ë¦¬[/]", 
        border_style="green"
    ))
    
    # ë§í¬ í…Œì´ë¸” ìƒì„±
    table = Table(title="ë£¨íŠ¸ ë””ë ‰í† ë¦¬ í•­ëª©")
    table.add_column("íŒŒì¼/ë””ë ‰í† ë¦¬", style="cyan")
    table.add_column("íƒ€ì…", style="magenta")
    table.add_column("í¬ê¸°", style="blue")
    table.add_column("ìˆ˜ì •ì¼", style="green")
    
    # í…Œì´ë¸”ì— ë°ì´í„° ì¶”ê°€
    for link in links[:5]:
        type_style = "[bold blue]ë””ë ‰í† ë¦¬[/]" if link.type == "directory" else "[yellow]íŒŒì¼[/]"
        table.add_row(
            link.filename, 
            type_style,
            link.size if link.size else "N/A",
            link.last_modified if link.last_modified else "N/A"
        )
        
    console.print(table)
    
    if len(links) > 5:
        console.print(f"[dim]... ê·¸ ì™¸ {len(links) - 5}ê°œ í•­ëª©ì´ ë” ìˆìŠµë‹ˆë‹¤.[/]")
    
    # ì§„í–‰ ìƒí™©ì„ í‘œì‹œí•  Progress ì»¨í…ìŠ¤íŠ¸ ìƒì„±
    progress_text = f"[bold green]íŒ¨í„´ '{pattern}'ìœ¼ë¡œ íŒŒì¼ ê²€ìƒ‰ ì¤‘...[/]"
    
    # ëª¨ë“  ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
    all_entries = []
    
    # í¬ë¡¤ë§ ì§„í–‰ ìƒíƒœ í‘œì‹œ ë° ê²°ê³¼ ìˆ˜ì§‘
    with Progress(
        SpinnerColumn(),
        TextColumn("[bold blue]{task.description}"),
        BarColumn(),
        TaskProgressColumn(),
        expand=True
    ) as progress:
        task = progress.add_task(progress_text, total=None)
        
        start_time = time.time()
        
        # ëª¨ë“  í•­ëª© ìˆ˜ì§‘ (ìµœëŒ€ ê¹Šì´ ì„¤ì •)
        async for entry in http_parser.walk(
            glob_pattern=pattern,
            max_depth=max_depth
        ):
            all_entries.append(entry)
            # í•­ëª© ë°œê²¬ì‹œë§ˆë‹¤ ì§„í–‰ ìƒíƒœ ì—…ë°ì´íŠ¸
            progress.update(task, description=f"[bold green]ê²€ìƒ‰ ì¤‘...[/] {len(all_entries)}ê°œ í•­ëª© ë°œê²¬")
        
        # ì™„ë£Œ ìƒíƒœë¡œ í‘œì‹œ
        progress.update(task, completed=True, description=f"[bold green]ê²€ìƒ‰ ì™„ë£Œ![/] {len(all_entries)}ê°œ í•­ëª© ë°œê²¬")
    
    # ê²°ê³¼ ì¶œë ¥
    end_time = time.time()
    duration = end_time - start_time
    
    # íƒ€ì…ë³„ í†µê³„
    file_count = sum(1 for entry in all_entries if entry.type == "file")
    dir_count = sum(1 for entry in all_entries if entry.type == "directory")
    
    # ìš”ì•½ ì •ë³´ íŒ¨ë„
    summary_panel = Panel(
        f"""[bold]ì´ {len(all_entries)}ê°œì˜ í•­ëª©ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.[/] (ì†Œìš” ì‹œê°„: {duration:.2f}ì´ˆ)
[cyan]íŒŒì¼:[/] {file_count}ê°œ, [magenta]ë””ë ‰í† ë¦¬:[/] {dir_count}ê°œ
[dim]ê²€ìƒ‰ íŒ¨í„´:[/] {pattern}, [dim]ìµœëŒ€ ê¹Šì´:[/] {max_depth}""",
        title="[bold green]ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½[/]", 
        border_style="green"
    )
    console.print(summary_panel)
    
    # ì¶œë ¥ í¬ë§·ì— ë”°ë¼ ê²°ê³¼ í‘œì‹œ
    if output_format == "table":
        # í…Œì´ë¸” í˜•ì‹ìœ¼ë¡œ ê²°ê³¼ í‘œì‹œ
        result_table = Table(title=f"ê²€ìƒ‰ ê²°ê³¼ (ìµœëŒ€ {max_display}ê°œ í‘œì‹œ)")
        result_table.add_column("ë²ˆí˜¸", style="dim")
        result_table.add_column("íŒŒì¼/ë””ë ‰í† ë¦¬", style="cyan")
        result_table.add_column("íƒ€ì…", style="magenta")
        result_table.add_column("í¬ê¸°", style="blue")
        result_table.add_column("ìˆ˜ì •ì¼", style="green")
        
        max_display_count = min(max_display, len(all_entries))
        for i, entry in enumerate(all_entries[:max_display_count]):
            type_icon = "ğŸ“" if entry.type == "directory" else "ğŸ“„"
            type_style = "[bold blue]ë””ë ‰í† ë¦¬[/]" if entry.type == "directory" else "[yellow]íŒŒì¼[/]"
            result_table.add_row(
                str(i+1),
                f"{type_icon} {entry.filename}", 
                type_style,
                entry.size if entry.size and entry.size != '-' else "N/A",
                entry.last_modified if entry.last_modified else "N/A"
            )
        
        console.print(result_table)
    
    elif output_format == "tree":
        # íŠ¸ë¦¬ í˜•ì‹ìœ¼ë¡œ ê²°ê³¼ í‘œì‹œ
        tree = Tree("[bold green]ê²€ìƒ‰ ê²°ê³¼[/]")
        
        # URLë³„ë¡œ í•­ëª© ê·¸ë£¹í™”
        entries_by_url = {}
        for entry in all_entries:
            base_url = '/'.join(entry.url.split('/')[:-1]) + '/'
            if base_url not in entries_by_url:
                entries_by_url[base_url] = []
            entries_by_url[base_url].append(entry)
        
        # íŠ¸ë¦¬ êµ¬ì„±
        count = 0
        max_display_count = min(max_display, len(all_entries))
        
        for url, entries in entries_by_url.items():
            # URL ê²½ë¡œë¥¼ ì§§ê²Œ í‘œì‹œ
            short_url = url.replace("https://", "").replace("http://", "")
            url_node = tree.add(f"[bold blue]{short_url}[/]")
            
            for entry in entries:
                if count >= max_display_count:
                    break
                    
                icon = "ğŸ“" if entry.type == "directory" else "ğŸ“„"
                entry_info = f"{icon} [cyan]{entry.filename}[/]"
                
                if entry.size and entry.size != '-':
                    entry_info += f" ([blue]{entry.size}[/])"
                    
                url_node.add(entry_info)
                count += 1
        
        console.print(tree)
    
    else:  # list í¬ë§·
        # ë¦¬ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ê²°ê³¼ í‘œì‹œ
        console.print("[bold green]ê²€ìƒ‰ ê²°ê³¼:[/]")
        
        max_display_count = min(max_display, len(all_entries))
        for i, entry in enumerate(all_entries[:max_display_count]):
            icon = "ğŸ“" if entry.type == "directory" else "ğŸ“„"
            console.print(f"{i+1}. {icon} [cyan]{entry.filename}[/] ([magenta]{entry.type}[/])")
            console.print(f"   URL: [dim]{entry.url}[/]")
            
            if entry.size and entry.size != '-':
                console.print(f"   í¬ê¸°: [blue]{entry.size}[/]")
                
            if entry.last_modified:
                console.print(f"   ìˆ˜ì •ì¼: [green]{entry.last_modified}[/]")
                
            console.print()
    
    # ë‚˜ë¨¸ì§€ í•­ëª©ì´ ìˆë‹¤ë©´ ë©”ì‹œì§€ ì¶œë ¥
    if len(all_entries) > max_display:
        console.print(f"[dim]... ê·¸ ì™¸ {len(all_entries) - max_display}ê°œ í•­ëª©ì´ ë” ìˆìŠµë‹ˆë‹¤.[/]")
    
    # ê²°ê³¼ë¥¼ íŒŒì¼ì— ì €ì¥
    if save_to_file:
        with open(save_to_file, 'w', encoding='utf-8') as f:
            f.write(f"# ê²€ìƒ‰ ê²°ê³¼\n")
            f.write(f"URL: {url}\n")
            f.write(f"íŒ¨í„´: {pattern}\n")
            f.write(f"ìµœëŒ€ ê¹Šì´: {max_depth}\n")
            f.write(f"ì´ í•­ëª© ìˆ˜: {len(all_entries)} (íŒŒì¼: {file_count}, ë””ë ‰í† ë¦¬: {dir_count})\n")
            f.write(f"ì†Œìš” ì‹œê°„: {duration:.2f}ì´ˆ\n\n")
            
            f.write("## ë°œê²¬ëœ í•­ëª© ëª©ë¡\n\n")
            for i, entry in enumerate(all_entries):
                f.write(f"{i+1}. {entry.filename} ({entry.type})\n")
                f.write(f"   URL: {entry.url}\n")
                if entry.size and entry.size != '-':
                    f.write(f"   í¬ê¸°: {entry.size}\n")
                if entry.last_modified:
                    f.write(f"   ìˆ˜ì •ì¼: {entry.last_modified}\n")
                f.write("\n")
        
        console.print(f"[bold green]ê²°ê³¼ê°€ '{save_to_file}' íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.[/]")

@app.command()
def crawl(
    url: str = typer.Option("https://www.statmt.org/europarl/v10/", "--url", "-u", help="í¬ë¡¤ë§í•  URL"),
    pattern: str = typer.Option("*.tsv.gz", "--pattern", "-p", help="íŒŒì¼ í•„í„°ë§ íŒ¨í„´ (ì˜ˆ: *.txt, *.tsv.gz)"),
    max_depth: int = typer.Option(3, "--max-depth", "-d", min=1, max=10, help="ìµœëŒ€ íƒìƒ‰ ê¹Šì´"),
    max_display: int = typer.Option(20, "--max-display", "-m", help="í™”ë©´ì— í‘œì‹œí•  ìµœëŒ€ í•­ëª© ìˆ˜"),
    output_format: str = typer.Option("list", "--format", "-f", help="ì¶œë ¥ í˜•ì‹ (list, table, tree)"),
    save_to_file: Optional[str] = typer.Option(None, "--output", "-o", help="ê²°ê³¼ë¥¼ ì €ì¥í•  íŒŒì¼ ê²½ë¡œ")
):
    """
    Apache HTTP ì„œë²„ ë””ë ‰í† ë¦¬ í˜ì´ì§€ë¥¼ í¬ë¡¤ë§í•˜ì—¬ íŒŒì¼ê³¼ ë””ë ‰í† ë¦¬ ëª©ë¡ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    """
    # ì¶œë ¥ í˜•ì‹ ê²€ì¦
    valid_formats = ["list", "table", "tree"]
    if output_format not in valid_formats:
        console.print(f"[bold red]ì˜¤ë¥˜:[/] ìœ íš¨í•˜ì§€ ì•Šì€ ì¶œë ¥ í˜•ì‹ '{output_format}'. ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ì„¸ìš”: {', '.join(valid_formats)}")
        raise typer.Exit(1)
    
    try:
        # ë¹„ë™ê¸° í•¨ìˆ˜ ì‹¤í–‰
        asyncio.run(crawl_and_display(url, pattern, max_depth, max_display, output_format, save_to_file))
    except Exception as e:
        console.print(f"[bold red]ì˜¤ë¥˜:[/] {e}")
        raise typer.Exit(1)

if __name__ == "__main__":
    app()
